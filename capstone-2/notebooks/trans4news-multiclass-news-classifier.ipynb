{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trans4News: Multiclass News Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T01:09:47.718238Z",
     "iopub.status.busy": "2024-01-20T01:09:47.717877Z",
     "iopub.status.idle": "2024-01-20T01:09:51.170824Z",
     "shell.execute_reply": "2024-01-20T01:09:51.170078Z",
     "shell.execute_reply.started": "2024-01-20T01:09:47.718211Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:01:14.420394Z",
     "iopub.status.busy": "2024-01-20T11:01:14.419820Z",
     "iopub.status.idle": "2024-01-20T11:01:17.591021Z",
     "shell.execute_reply": "2024-01-20T11:01:17.590086Z",
     "shell.execute_reply.started": "2024-01-20T11:01:14.420351Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from seaborn) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.1-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:00:38.966168Z",
     "iopub.status.busy": "2024-01-20T11:00:38.965762Z",
     "iopub.status.idle": "2024-01-20T11:00:48.589523Z",
     "shell.execute_reply": "2024-01-20T11:00:48.588770Z",
     "shell.execute_reply.started": "2024-01-20T11:00:38.966142Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.20.2 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T14:20:49.659552Z",
     "iopub.status.busy": "2024-01-20T14:20:49.659159Z",
     "iopub.status.idle": "2024-01-20T14:20:49.665126Z",
     "shell.execute_reply": "2024-01-20T14:20:49.664144Z",
     "shell.execute_reply.started": "2024-01-20T14:20:49.659525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:01:48.271454Z",
     "iopub.status.busy": "2024-01-20T11:01:48.270894Z",
     "iopub.status.idle": "2024-01-20T11:01:48.275169Z",
     "shell.execute_reply": "2024-01-20T11:01:48.274341Z",
     "shell.execute_reply.started": "2024-01-20T11:01:48.271420Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:00.665783Z",
     "iopub.status.busy": "2024-01-20T11:29:00.665392Z",
     "iopub.status.idle": "2024-01-20T11:29:00.671329Z",
     "shell.execute_reply": "2024-01-20T11:29:00.670265Z",
     "shell.execute_reply.started": "2024-01-20T11:29:00.665758Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        self.seed = 42\n",
    "        self.exp_name = \"Trans4News: Multiclass News Classifier\"\n",
    "        self.model_name = \"bert\"\n",
    "        self.pre_model = \"bert-base-uncased\"\n",
    "        self.pre_distil_model = \"distilbert-base-uncased\"\n",
    "        self.train_PATH = \"train.csv\"\n",
    "        self.test_PATH = \"test.csv\"\n",
    "        self.max_length = 50\n",
    "        self.num_workers = 0\n",
    "        self.epochs = 5\n",
    "        self.patience = 2\n",
    "        self.num_classes = 4\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = 32\n",
    "        self.shuffle_train = True\n",
    "        self.dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:01.042186Z",
     "iopub.status.busy": "2024-01-20T11:29:01.041476Z",
     "iopub.status.idle": "2024-01-20T11:29:01.047117Z",
     "shell.execute_reply": "2024-01-20T11:29:01.046357Z",
     "shell.execute_reply.started": "2024-01-20T11:29:01.042155Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> SEEDING DONE\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(\">> SEEDING DONE\")\n",
    "\n",
    "\n",
    "set_seed(Config().seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:01.323159Z",
     "iopub.status.busy": "2024-01-20T11:29:01.322746Z",
     "iopub.status.idle": "2024-01-20T11:29:01.330581Z",
     "shell.execute_reply": "2024-01-20T11:29:01.329708Z",
     "shell.execute_reply.started": "2024-01-20T11:29:01.323127Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save all infos about training data in one place\n",
    "class DataContext:\n",
    "    def __init__(self, config) -> None:\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(config.pre_model)\n",
    "        self.distil_bert_tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "            config.pre_distil_model\n",
    "        )\n",
    "        self.vectorizer = None\n",
    "        self.train_dataset = None\n",
    "        self.valid_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.train_dataloader = None\n",
    "        self.valid_dataloader = None\n",
    "        self.test_dataloader = None\n",
    "        self.df_train = None\n",
    "        self.df_test = None\n",
    "        self.df = None\n",
    "\n",
    "    # preprocessing method for all texts\n",
    "    def preprocess_texts(self) -> None:\n",
    "        preprocessed_texts_ls = []\n",
    "        for ix, row in self.df.iterrows():\n",
    "            text = row.text\n",
    "\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            # Remove URLs\n",
    "            text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "            # Remove mentions and hashtags\n",
    "            text = re.sub(r\"@\\w+|\\#\", \"\", text)\n",
    "\n",
    "            # Remove special characters and punctuation\n",
    "            text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "            # Remove spaces\n",
    "            text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "            # Remove unnecessary dots\n",
    "            text = re.sub(r\"\\.{2,}\", \".\", text)\n",
    "\n",
    "            # Remove dots at the beginning or end of the sentence\n",
    "            text = text.strip(\".\")\n",
    "\n",
    "            # Remove spaces at the beginning or end of the sentence\n",
    "            text = text.strip(\" \")\n",
    "\n",
    "            preprocessed_texts_ls.append(text)\n",
    "\n",
    "        # Create new columns in our main df\n",
    "        self.df[\"preprocessed_news\"] = preprocessed_texts_ls\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:01.478949Z",
     "iopub.status.busy": "2024-01-20T11:29:01.478224Z",
     "iopub.status.idle": "2024-01-20T11:29:01.485056Z",
     "shell.execute_reply": "2024-01-20T11:29:01.484256Z",
     "shell.execute_reply.started": "2024-01-20T11:29:01.478914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        # Subtract 1 from each value in the 'labels' column\n",
    "        self.dataframe[\"labels\"] = self.dataframe[\"labels\"] - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if \"token_type_ids\" in self.dataframe.columns.tolist():\n",
    "            sample = {\n",
    "                \"input_ids\": self.dataframe[\"input_ids\"].iloc[idx],\n",
    "                \"attention_mask\": self.dataframe[\"attention_mask\"].iloc[idx],\n",
    "                \"token_type_ids\": self.dataframe[\"token_type_ids\"].iloc[idx],\n",
    "                \"label\": torch.tensor(\n",
    "                    self.dataframe[\"labels\"].iloc[idx], dtype=torch.long\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                \"input_ids\": self.dataframe[\"input_ids\"].iloc[idx],\n",
    "                \"attention_mask\": self.dataframe[\"attention_mask\"].iloc[idx],\n",
    "                \"label\": torch.tensor(\n",
    "                    self.dataframe[\"labels\"].iloc[idx], dtype=torch.long\n",
    "                ),\n",
    "            }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:01.613810Z",
     "iopub.status.busy": "2024-01-20T11:29:01.613405Z",
     "iopub.status.idle": "2024-01-20T11:29:01.619995Z",
     "shell.execute_reply": "2024-01-20T11:29:01.619101Z",
     "shell.execute_reply.started": "2024-01-20T11:29:01.613784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_tokenization(config, context, model_name):\n",
    "    # Tokenize each news in the DataFrame\n",
    "    def tokenize_news(news_text):\n",
    "        if model_name == \"BertToken\":\n",
    "            tokens = context.bert_tokenizer(\n",
    "                news_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=config.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "                tokens[key] = torch.LongTensor((tokens[key]))\n",
    "        elif model_name == \"DistilBertToken\":\n",
    "            tokens = context.distil_bert_tokenizer(\n",
    "                news_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=config.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            for key in [\"input_ids\", \"attention_mask\"]:\n",
    "                tokens[key] = torch.LongTensor((tokens[key]))\n",
    "        return tokens\n",
    "\n",
    "    tokenized_news = context.df[\"preprocessed_news\"].apply(tokenize_news)\n",
    "    return tokenized_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:01.763057Z",
     "iopub.status.busy": "2024-01-20T11:29:01.762449Z",
     "iopub.status.idle": "2024-01-20T11:29:01.776097Z",
     "shell.execute_reply": "2024-01-20T11:29:01.775398Z",
     "shell.execute_reply.started": "2024-01-20T11:29:01.763031Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset can be either train, test or valid\n",
    "def load_dataset(config) -> DataContext:\n",
    "    context = DataContext(config)\n",
    "\n",
    "    # Read the dataset from config.train_PATH and config.test_PATH\n",
    "    context.df_train = pd.read_csv(config.train_PATH, encoding=\"utf-8\")\n",
    "\n",
    "    context.df_test = pd.read_csv(config.test_PATH, encoding=\"utf-8\")\n",
    "\n",
    "    # Merge DataFrames\n",
    "    context.df = pd.concat([context.df_train, context.df_test], ignore_index=True)\n",
    "\n",
    "    # Assuming df is your DataFrame\n",
    "    context.df[\"text\"] = context.df[\"Title\"] + \" \" + context.df[\"Description\"]\n",
    "\n",
    "    # Rename 'Class Index' to 'label'\n",
    "    context.df = context.df.rename(columns={\"Class Index\": \"labels\"})\n",
    "\n",
    "    # Print a log\n",
    "    print(\">> CSV LOADING DONE.\")\n",
    "\n",
    "    # Preprocess and create encodings for the dataset\n",
    "    context.preprocess_texts()\n",
    "\n",
    "    # Print a log\n",
    "    print(\">> DATA PREPROCESSING DONE.\")\n",
    "\n",
    "    # Tokenize each news and add the 'input_ids', 'attention_mask', and 'token_type_ids' columns\n",
    "    tokenized_news = do_tokenization(\n",
    "        config, context, \"DistilBertToken\"\n",
    "    )  # you can change it here for BertToken\n",
    "    context.df = pd.concat([context.df, pd.DataFrame(tokenized_news.tolist())], axis=1)\n",
    "\n",
    "    # Print a log\n",
    "    print(\">> DATA TOKENIZATION DONE.\")\n",
    "\n",
    "    # Create a custom dataset instance\n",
    "    dataset = CustomDataset(context.df)\n",
    "\n",
    "    # Define the sizes for train, validation, and test sets\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(config.seed),\n",
    "    )\n",
    "\n",
    "    # Create context dataset for hyperparameter tuning\n",
    "    context.train_dataset = train_dataset\n",
    "    context.valid_dataset = val_dataset\n",
    "    context.test_dataset = test_dataset\n",
    "\n",
    "    # Create data loaders for train, validation, and test sets\n",
    "    context.train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=config.shuffle_train,\n",
    "        num_workers=config.num_workers,\n",
    "    )\n",
    "    context.valid_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "    )\n",
    "    context.test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "    )\n",
    "\n",
    "    # Print a log\n",
    "    print(\">> DATALOADER AND VALIDATION FRAMEWORK CREATED.\")\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:02.425360Z",
     "iopub.status.busy": "2024-01-20T11:29:02.424961Z",
     "iopub.status.idle": "2024-01-20T11:29:02.429042Z",
     "shell.execute_reply": "2024-01-20T11:29:02.428306Z",
     "shell.execute_reply.started": "2024-01-20T11:29:02.425333Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_obj = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:29:02.855937Z",
     "iopub.status.busy": "2024-01-20T11:29:02.855528Z",
     "iopub.status.idle": "2024-01-20T11:31:08.594278Z",
     "shell.execute_reply": "2024-01-20T11:31:08.593324Z",
     "shell.execute_reply.started": "2024-01-20T11:29:02.855910Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> CSV LOADING DONE.\n",
      ">> DATA PREPROCESSING DONE.\n",
      ">> DATA TOKENIZATION DONE.\n",
      ">> DATALOADER AND VALIDATION FRAMEWORK CREATED.\n"
     ]
    }
   ],
   "source": [
    "context = load_dataset(config_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:31:08.596219Z",
     "iopub.status.busy": "2024-01-20T11:31:08.595789Z",
     "iopub.status.idle": "2024-01-20T11:31:08.732260Z",
     "shell.execute_reply": "2024-01-20T11:31:08.731369Z",
     "shell.execute_reply.started": "2024-01-20T11:31:08.596194Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127600 entries, 0 to 127599\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   labels             127600 non-null  int64 \n",
      " 1   Title              127600 non-null  object\n",
      " 2   Description        127600 non-null  object\n",
      " 3   text               127600 non-null  object\n",
      " 4   preprocessed_news  127600 non-null  object\n",
      " 5   attention_mask     127600 non-null  object\n",
      " 6   input_ids          127600 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "context.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:19:53.491663Z",
     "iopub.status.busy": "2024-01-20T10:19:53.491342Z",
     "iopub.status.idle": "2024-01-20T10:19:53.496238Z",
     "shell.execute_reply": "2024-01-20T10:19:53.495378Z",
     "shell.execute_reply.started": "2024-01-20T10:19:53.491638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def showLengthCount(context):\n",
    "    df = context.df.copy()\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        df.loc[i, \"length\"] = len(df.loc[i, \"preprocessed_news\"].split())\n",
    "    print(\"Count of news : \", len(df))\n",
    "    labels = list(df.labels.unique())\n",
    "    for label in labels:\n",
    "        df_label = df[df[\"labels\"] == label]\n",
    "        print(f\"Max length for class: {label} is : {df_label.length.unique().max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T10:19:53.498006Z",
     "iopub.status.busy": "2024-01-20T10:19:53.497717Z",
     "iopub.status.idle": "2024-01-20T10:20:12.171515Z",
     "shell.execute_reply": "2024-01-20T10:20:12.170431Z",
     "shell.execute_reply.started": "2024-01-20T10:19:53.497982Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of news :  127600\n",
      "Max length for class: 2 is : 134.0\n",
      "Max length for class: 3 is : 177.0\n",
      "Max length for class: 1 is : 148.0\n",
      "Max length for class: 0 is : 143.0\n"
     ]
    }
   ],
   "source": [
    "showLengthCount(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T11:58:00.025410Z",
     "iopub.status.busy": "2024-01-20T11:58:00.025013Z",
     "iopub.status.idle": "2024-01-20T14:05:24.188206Z",
     "shell.execute_reply": "2024-01-20T14:05:24.187379Z",
     "shell.execute_reply.started": "2024-01-20T11:58:00.025383Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518e8f71f9114b179a818484c92763d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efac098e7ec54746bad6d0a15135f10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 1379.465988598764, Validation Loss: 0.3294312631670939, Validation Accuracy: 0.8881922675026124, Validation F1: 0.8881922675026124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fd0bd8ad164ed1bb1d1fc6e22bd293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4675f23333443ba3bfb4ead546112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 778.7553430832922, Validation Loss: 0.2871452818417887, Validation Accuracy: 0.9029258098223616, Validation F1: 0.9029258098223615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d090ff77ce424d51983e9d1d1af01643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fbef78018c4b2ab01aa62e55362aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 634.8332336228341, Validation Loss: 0.2677471737046174, Validation Accuracy: 0.9102403343782655, Validation F1: 0.9102403343782655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4139d6c6e3f4cfcadbce04687e65e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b651ec5a9049efa39fc969731afec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 537.0085411891341, Validation Loss: 0.272242692437564, Validation Accuracy: 0.9078369905956113, Validation F1: 0.9078369905956113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f055a910f346cca0db98ed8e612047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3978a78d444eccae15e59ed3d2565b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 468.593920183368, Validation Loss: 0.3084229033017447, Validation Accuracy: 0.9032915360501568, Validation F1: 0.9032915360501568\n",
      "Early stopping. Training stopped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290d7a3cf4ab403cbaeca215a63d300e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fe020b51c6463ebf002a9c61b3a24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 3881.613509297371, Validation Loss: 1.3863279548829703, Validation Accuracy: 0.24869383490073146, Validation F1: 0.24869383490073146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cde5a0617d4624b253e412f51c5650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7b12cd2b0d44439065487e1c506676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 3872.951962351799, Validation Loss: 1.3863223424936972, Validation Accuracy: 0.24843260188087773, Validation F1: 0.24843260188087773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dbbca410e9450b857a9ba90a84be00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4416ee1b08442fabd171337107a4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 3871.3625857830048, Validation Loss: 1.3863081145963208, Validation Accuracy: 0.24869383490073146, Validation F1: 0.24869383490073146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720c47e04f96418cbcc11043cafe2bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445ce50ac0f044c6a4fe1d3ac118024e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 3870.7452610731125, Validation Loss: 1.386400209643407, Validation Accuracy: 0.24869383490073146, Validation F1: 0.24869383490073146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05192fb191048f39aab67f412a15e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/2792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce9b3c6d239418fa8520b15be89d109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation::   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 3871.361182808876, Validation Loss: 1.386343927932701, Validation Accuracy: 0.24843260188087773, Validation F1: 0.24843260188087773\n",
      "Early stopping. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Tensorboard SummaryWriter for logging\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for model_name in [\n",
    "    \"DistilBertForSequenceClassification\",\n",
    "    \"BertForSequenceClassification\",\n",
    "]:\n",
    "    for lr in [1e-5, 1e-3]:\n",
    "        # Model definition\n",
    "        if model_name == \"BertForSequenceClassification\":\n",
    "            # Bert configuration\n",
    "            config_bert = BertConfig.from_pretrained(\n",
    "                config_obj.pre_model,\n",
    "                num_labels=config_obj.num_classes,\n",
    "            )\n",
    "            model = BertForSequenceClassification(config_bert)\n",
    "        elif model_name == \"DistilBertForSequenceClassification\":\n",
    "            # DistilBert configuration\n",
    "            config_distilbert = DistilBertConfig.from_pretrained(\n",
    "                config_obj.pre_model,\n",
    "                num_labels=config_obj.num_classes,\n",
    "            )\n",
    "            model = DistilBertForSequenceClassification(config_distilbert)\n",
    "\n",
    "        # Model to device\n",
    "        model = model.to(config_obj.device)\n",
    "\n",
    "        # Define optimizer and criterion\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training loop\n",
    "        best_val_loss = float(\"inf\")\n",
    "        early_stopping_counter = 0\n",
    "        for epoch in range(config_obj.epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in tqdm(\n",
    "                context.train_dataloader, desc=f\"Epoch {epoch + 1}/{config_obj.epochs}\"\n",
    "            ):\n",
    "                input_ids = batch[\"input_ids\"].squeeze(1).to(config_obj.device)\n",
    "                attention_mask = (\n",
    "                    batch[\"attention_mask\"].squeeze(1).to(config_obj.device)\n",
    "                )\n",
    "                labels = batch[\"label\"].to(config_obj.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if model_name == \"DistilBertForSequenceClassification\":\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                elif model_name == \"BertForSequenceClassification\":\n",
    "                    token_type_ids = (\n",
    "                        batch[\"token_type_ids\"].squeeze(1).to(config_obj.device)\n",
    "                    )\n",
    "                    outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(context.valid_dataloader, desc=f\"Validation:\"):\n",
    "                    input_ids = batch[\"input_ids\"].squeeze(1).to(config_obj.device)\n",
    "                    attention_mask = (\n",
    "                        batch[\"attention_mask\"].squeeze(1).to(config_obj.device)\n",
    "                    )\n",
    "                    labels = batch[\"label\"].to(config_obj.device)\n",
    "\n",
    "                    if model_name == \"DistilBertForSequenceClassification\":\n",
    "                        outputs = model(input_ids, attention_mask)\n",
    "                    elif model_name == \"BertForSequenceClassification\":\n",
    "                        token_type_ids = (\n",
    "                            batch[\"token_type_ids\"].squeeze(1).to(config_obj.device)\n",
    "                        )\n",
    "                        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    y_true.extend(labels.cpu().numpy())\n",
    "                    y_pred.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "\n",
    "            val_loss /= len(context.valid_dataloader)\n",
    "            accuracy_val = accuracy_score(y_true, y_pred)\n",
    "            f1_val = f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{config_obj.epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}, Validation Accuracy: {accuracy_val}, Validation F1: {f1_val}\"\n",
    "            )\n",
    "\n",
    "            # Tensorboard logging\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/Val\", val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Val\", accuracy_val, epoch)\n",
    "            writer.add_scalar(\"F1/Val\", f1_val, epoch)\n",
    "\n",
    "            # Early stopping and model checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                torch.save(\n",
    "                    model.state_dict(), f\"best_model_{model_name}_{lr}.bin\"\n",
    "                )  # Save the best model\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "            if early_stopping_counter >= config_obj.patience:\n",
    "                print(\"Early stopping. Training stopped.\")\n",
    "                break\n",
    "\n",
    "# Close the Tensorboard SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "DistilBertForSequenceClassification performed well with 1e-5 learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T14:29:56.010349Z",
     "iopub.status.busy": "2024-01-20T14:29:56.009940Z",
     "iopub.status.idle": "2024-01-20T14:29:58.131480Z",
     "shell.execute_reply": "2024-01-20T14:29:58.130727Z",
     "shell.execute_reply.started": "2024-01-20T14:29:56.010321Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the DistilBERT model with the number of classes in your dataset\n",
    "config_distilbert = DistilBertConfig.from_pretrained(\n",
    "    config_obj.pre_model,\n",
    "    num_labels=config_obj.num_classes,\n",
    ")\n",
    "best_model = DistilBertForSequenceClassification(config_distilbert)\n",
    "\n",
    "\n",
    "# Load the trained model state_dict\n",
    "best_model.load_state_dict(torch.load(\"best_model_DistilBertForSequenceClassification_1e-05.bin\"))\n",
    "\n",
    "best_model = best_model.to(config_obj.device)\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T14:32:01.639188Z",
     "iopub.status.busy": "2024-01-20T14:32:01.638780Z",
     "iopub.status.idle": "2024-01-20T14:32:48.542067Z",
     "shell.execute_reply": "2024-01-20T14:32:48.541168Z",
     "shell.execute_reply.started": "2024-01-20T14:32:01.639161Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5fbabd9c534370a3c485ef4fa2785c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9108672936259143\n",
      "Test F1 Score: 0.9108672936259143\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      4792\n",
      "           1       0.95      0.98      0.97      4767\n",
      "           2       0.88      0.88      0.88      4843\n",
      "           3       0.88      0.90      0.89      4738\n",
      "\n",
      "    accuracy                           0.91     19140\n",
      "   macro avg       0.91      0.91      0.91     19140\n",
      "weighted avg       0.91      0.91      0.91     19140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "best_model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(\n",
    "                context.test_dataloader, desc=f\"Epoch {epoch + 1}/{config_obj.epochs}\"\n",
    "            ):\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(config_obj.device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(config_obj.device)\n",
    "        labels = batch['label'].to(config_obj.device)\n",
    "        \n",
    "        outputs = best_model(input_ids, attention_mask)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate and print metrics\n",
    "accuracy_test = accuracy_score(y_true, y_pred)\n",
    "f1_test = f1_score(y_true, y_pred, average='micro')\n",
    "classification_report_test = classification_report(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n",
    "print(f\"Test F1 Score: {f1_test}\")\n",
    "print(\"Classification Report:\\n\", classification_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 612351,
     "sourceId": 1095715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
